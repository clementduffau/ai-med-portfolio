{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e270f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np \n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import torch\n",
    "parent_root = Path.cwd().parent\n",
    "project_root = os.path.join(parent_root, \"src\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "from pytorch_lightning import seed_everything\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt\n",
    "import hydra\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from datamodule import DataModule_llm\n",
    "from lightning_module import llm_pl\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e234db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"..\") / \"src\"\n",
    "data_dir = out_dir / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fefaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model token id </s>\n",
      "ratio trainable parameters 0.40791125334440875\n"
     ]
    }
   ],
   "source": [
    "@hydra.main(config_path=\"config\", config_name=\"config_seg\", version_base=\"1.3\")\n",
    "def main(cfg: DictConfig):\n",
    "\n",
    "    seed_everything(cfg.get(\"seed\", 42), workers=True)\n",
    "\n",
    "    cfg_data = cfg.data\n",
    "    cfg_model = cfg.model\n",
    "    cfg_lora = cfg_model.lora\n",
    "\n",
    "    datamodule = DataModule_llm(cfg_data)\n",
    "    model = llm_pl(cfg_model, cfg_lora)\n",
    "    return model, datamodule\n",
    "\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "config_path = os.path.join(out_dir, \"config\")\n",
    "\n",
    "with initialize(version_base=None, config_path=config_path, job_name=\"nb\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    model, datamodule = main(cfg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d774ba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 224562/224562 [00:52<00:00, 4251.82 examples/s]\n",
      "Map: 100%|██████████| 4583/4583 [00:01<00:00, 4389.15 examples/s]\n",
      "Map: 100%|██████████| 4677/4677 [00:00<00:00, 5074.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "datamodule.setup(stage=\"test\")\n",
    "test_loader = datamodule.test_dataloader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579ef7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model token id </s>\n",
      "ratio trainable parameters 0.40791125334440875\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = out_dir / \"checkpoints/best.ckpt\"\n",
    "model = llm_pl.load_from_checkpoint(\n",
    "    ckpt_path,\n",
    "    weights_only = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be76e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm_pl(\n",
       "  (model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(32000, 2048)\n",
       "          (layers): ModuleList(\n",
       "            (0-21): 22 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"bf16\")\n",
    "model = accelerator.prepare(\n",
    "    model\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf4ebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loss:   0%|          | 0/585 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Test loss: 100%|██████████| 585/585 [00:42<00:00, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 1.018826152707034, 'test_ppl': 2.769941366667132, 'n_tokens': 20439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(test_loader, desc=\"Test loss\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        out = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        loss = out.loss  \n",
    "\n",
    "        # count how many tokens contribute to loss (labels != -100)\n",
    "        n_tokens = (batch[\"labels\"] != -100).sum().item()\n",
    "\n",
    "        total_loss += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "avg_loss = total_loss / max(total_tokens, 1)\n",
    "ppl = math.exp(avg_loss)\n",
    "\n",
    "print({\"test_loss\": avg_loss, \"test_ppl\": ppl, \"n_tokens\": total_tokens})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5c9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, prompt_text, device, max_new_tokens = 128):\n",
    "    inputs = tokenizer(prompt_text, return_tensors = \"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = max_new_tokens,\n",
    "            do_sample = False,\n",
    "            temperature = 1.0,\n",
    "            top_p = 1.0,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5608 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK: qa_medmcqa\n",
      "\n",
      "PROMPT:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: medical_mcq\n",
      "User: An 18-year-old man moves from sea level to an elevation of 2,400 m to train as a skier. The increased requirement for oxygen delivery to tissues at the higher elevation stimulates the synthesis of a renal hormone (erythropoietin), which targets hematopoietic stem cells in the bone marrow. Erythropoietin promotes the survival of early erythroid progenitor cells primarily through which of the following mechanisms?\n",
      "\n",
      "Options:\n",
      "A. Altered cell-matrix adhesion\n",
      "B. Downregulation of p53\n",
      "C. Enhanced glucose uptake\n",
      "D. Inhibition of apoptosis\n",
      "Reply with: Final: <A|B|C|D>\n",
      "Assistant:\n",
      "\n",
      "GOLD:\n",
      " Final: D\n",
      "\n",
      "PRED:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: medical_mcq\n",
      "User: An 18-year-old man moves from sea level to an elevation of 2,400 m to train as a skier. The increased requirement for oxygen delivery to tissues at the higher elevation stimulates the synthesis of a renal hormone (erythropoietin), which targets hematopoietic stem cells in the bone marrow. Erythropoietin promotes the survival of early erythroid progenitor cells primarily through which of the following mechanisms?\n",
      "\n",
      "Options:\n",
      "A. Altered cell-matrix adhesion\n",
      "B. Downregulation of p53\n",
      "C. Enhanced glucose uptake\n",
      "D. Inhibition of apoptosis\n",
      "Reply with: Final: <A|B|C|D>\n",
      "Assistant:Final: C\n",
      "\n",
      "Task: erythropoietin\n",
      "User: Erythro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK: summarization_pubmed\n",
      "\n",
      "PROMPT:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: summarization\n",
      "User: Summarize the following biomedical article into a concise abstract:\n",
      "\n",
      "the study was designed as a prospective , observational , referral center cohort study of consecutive diabetic patients who underwent pci . \n",
      " all incident cases of cli were recorded and followed within a structured , collaborative framework ( diabetologist , foot care specialist , vascular surgeon , interventional cardiologist ) . \n",
      " this model of strict collaboration among different professional figures with a dedicated pathway for diabetic patients and early , aggressive attempts at endovascular revascularization , has been previously described ( 21 ) and demonstrated to result in a very low amputation rate . \n",
      " consecutive diabetic patients undergoing pci with or without stent implantation for either acute coronary syndrome or stable coronary disease between july 2002 and may 2007 at the cardiovascular department of san donato hospital ( arezzo , italy ) were enrolled . \n",
      " this is a pci and peripheral interventions center serving a population of 350,000 in central\n",
      "\n",
      "GOLD:\n",
      " objectivedevelopment of critical limb ischemia ( cli ) has been reported as an independent predictor of cardiac mortality in diabetic patients . \n",
      " we aimed to determine whether cli , managed in a structured setting of close collaboration between different vascular specialists and treated with early endovascular intervention , has any impact on long - term cardiac mortality of diabetic patients initially presenting with symptomatic coronary artery disease ( cad).research design and methodswe designed a prospective observational study of 764 consecutive diabetic patients undergoing percutaneous \n",
      "\n",
      "PRED:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: summarization\n",
      "User: Summarize the following biomedical article into a concise abstract:\n",
      "\n",
      "the study was designed as a prospective , observational , referral center cohort study of consecutive diabetic patients who underwent pci . \n",
      " all incident cases of cli were recorded and followed within a structured , collaborative framework ( diabetologist , foot care specialist , vascular surgeon , interventional cardiologist ) . \n",
      " this model of strict collaboration among different professional figures with a dedicated pathway for diabetic patients and early , aggressive attempts at endovascular revascularization , has been previously described ( 21 ) and demonstrated to result in a very low amputation rate . \n",
      " consecutive diabetic patients undergoing pci with or without stent implantation for either acute coronary syndrome or stable coronary disease between july 2002 and may 2007 at the cardiovascular department of san donato hospital ( arezzo , italy ) were enrolled . \n",
      " this is a pci and peripheral interventions center serving a population of 350,000 in central\n",
      "\n",
      "================================================================================\n",
      "TASK: qa_medmcqa\n",
      "\n",
      "PROMPT:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: medical_mcq\n",
      "User: Arrange the enzymes used to conve propionyl CoA to glucose in sequence: A. Methyl malonyl CoA racemase B. Succinate Thiokinase C. Methyl malonyl CoA mutase D. Propionyl CoA carboxylase\n",
      "\n",
      "Options:\n",
      "A. D- B- C-A\n",
      "B. D- A- C-B\n",
      "C. A- D- B-C\n",
      "D. B- A- D-C\n",
      "Reply with: Final: <A|B|C|D>\n",
      "Assistant:\n",
      "\n",
      "GOLD:\n",
      " Final: B\n",
      "\n",
      "PRED:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: medical_mcq\n",
      "User: Arrange the enzymes used to conve propionyl CoA to glucose in sequence: A. Methyl malonyl CoA racemase B. Succinate Thiokinase C. Methyl malonyl CoA mutase D. Propionyl CoA carboxylase\n",
      "\n",
      "Options:\n",
      "A. D- B- C-A\n",
      "B. D- A- C-B\n",
      "C. A- D- B-C\n",
      "D. B- A- D-C\n",
      "Reply with: Final: <A|B|C|D>\n",
      "Assistant:Final: A\n",
      "\n",
      "Medical Term: A-B-C-D\n",
      "Medical Term\n",
      "\n",
      "================================================================================\n",
      "TASK: qa_medmcqa\n",
      "\n",
      "PROMPT:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: medical_mcq\n",
      "User: The commonest site of diveiculosis is:\n",
      "\n",
      "Options:\n",
      "A. Ascending colon\n",
      "B. Transverse colon\n",
      "C. Descendin gcolon\n",
      "D. Sigmoid colon\n",
      "Reply with: Final: <A|B|C|D>\n",
      "Assistant:\n",
      "\n",
      "GOLD:\n",
      " Final: D\n",
      "\n",
      "PRED:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: medical_mcq\n",
      "User: The commonest site of diveiculosis is:\n",
      "\n",
      "Options:\n",
      "A. Ascending colon\n",
      "B. Transverse colon\n",
      "C. Descendin gcolon\n",
      "D. Sigmoid colon\n",
      "Reply with: Final: <A|B|C|D>\n",
      "Assistant:Final: D\n",
      "\n",
      "Task: medical_mcq\n",
      "User: What is the most common site\n",
      "\n",
      "================================================================================\n",
      "TASK: summarization_pubmed\n",
      "\n",
      "PROMPT:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: summarization\n",
      "User: Summarize the following biomedical article into a concise abstract:\n",
      "\n",
      "thermal ablation of lung tumours is more frequently used as a primary mode of treatment in adults , its efficacy having been proven in several studies [ 1 , 2 ] . \n",
      " the benefit of this technique is that it is less invasive and spares healthy lung tissue , reducing the overall complication rate and costs and allowing for repeated treatments . \n",
      " the use of ablative lung procedures in a paediatric setting has been reported , but there are only a few publications on this subject . in previous studies , \n",
      " microwave energy has several advantages over radiofrequency , such as its speed , no reliance on tissue conductivity , which can be a problem in the air - filled lungs , and the absence of tissue charring which reduces the effect of radiofrequency energy . \n",
      " targeting of lung tumours is dependent on non - ultrasound modalities as most lung tumours are impossible to visualize behind air - filled alveoli . \n",
      " computed tomography is the primary choice but has some negativ\n",
      "\n",
      "GOLD:\n",
      " this is a case report of microwave energy being used to ablate an inoperable metastasis of a wilms tumour in a 6-year - old boy using state - of - the - art navigated computed tomography targeting and high - frequency jet ventilation to reduce organ displacement and the potential risk of procedure - related pneumothorax . \n",
      " after the ablation , the young boy had high - dose chemotherapy followed by an autologous stem cell transplantation with rapid reduction of three recurrent right - sided lung metastases .\n",
      "\n",
      "PRED:\n",
      " System: You are a medical assistant. Answer based on the provided context. If the context is insufficient, say you don't know.\n",
      "Task: summarization\n",
      "User: Summarize the following biomedical article into a concise abstract:\n",
      "\n",
      "thermal ablation of lung tumours is more frequently used as a primary mode of treatment in adults , its efficacy having been proven in several studies [ 1 , 2 ] . \n",
      " the benefit of this technique is that it is less invasive and spares healthy lung tissue , reducing the overall complication rate and costs and allowing for repeated treatments . \n",
      " the use of ablative lung procedures in a paediatric setting has been reported , but there are only a few publications on this subject . in previous studies , \n",
      " microwave energy has several advantages over radiofrequency , such as its speed , no reliance on tissue conductivity , which can be a problem in the air - filled lungs , and the absence of tissue charring which reduces the effect of radiofrequency energy . \n",
      " targeting of lung tumours is dependent on non - ultrasound modalities as most lung tumours are impossible to visualize behind air - filled alveoli . \n",
      " computed tomography is the primary choice but has some negativ\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=True)\n",
    "ds = load_dataset(\"json\", data_files={\"train\": str(\"/home/clement/mnt/ssd_nvme/ai-med-portfolio/projects/dl/llm-med-project/src/data/train.jsonl\"), \n",
    "                                                   \"validation\": str(\"/home/clement/mnt/ssd_nvme/ai-med-portfolio/projects/dl/llm-med-project/src/data/val.jsonl\"), \n",
    "                                                   \"test\": str(\"/home/clement/mnt/ssd_nvme/ai-med-portfolio/projects/dl/llm-med-project/src/data/test.jsonl\")})\n",
    "\n",
    "idxs = random.sample(range(len(ds[\"test\"])), 5)\n",
    "\n",
    "for idx in idxs:\n",
    "    ex = ds[\"test\"][idx]\n",
    "    prompt = ex[\"input\"]\n",
    "    gold = ex[\"output\"]\n",
    "    task = ex.get(\"task\", \"unknown\")\n",
    "    if task == \"qa_medmcqa\":\n",
    "        pred = generate_answer(model, tokenizer, prompt, device, max_new_tokens=30)\n",
    "    else:\n",
    "        pred = generate_answer(model, tokenizer, prompt, device, max_new_tokens = 128)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TASK:\", task)\n",
    "    print(\"\\nPROMPT:\\n\", prompt[:1200])\n",
    "    print(\"\\nGOLD:\\n\", gold[:600])\n",
    "    print(\"\\nPRED:\\n\", pred[:1200])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-med-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
